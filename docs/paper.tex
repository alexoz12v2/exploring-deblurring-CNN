% Compilation commands
% lualatex paper.tex
% makeglossaries paper
% biber paper
% lualatex paper.tex

\documentclass[a4paper,10pt,twocolumn]{article}

% Page margins and layout
\usepackage[margin=2cm]{geometry}
\usepackage{multicol}
\setlength{\columnsep}{0.7cm}

% glossary
\usepackage[acronym, toc]{glossaries}
\makeglossaries
\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{references.bib}

% Font and encoding
% \usepackage[utf8]{inputenc} % only pdflatex
% \usepackage[T1]{fontenc} % only pdflatex
\usepackage{lmodern}
\usepackage[italian]{babel}

% Math
\usepackage{amsmath, amssymb, bm}

% Graphics and plots
\usepackage{mdframed} % boxing di figures con mdframed
\usepackage{placeins} % \FloatBarrier
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% Tables
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{longtable}

% Lists
\usepackage{enumitem}
\setlist{nosep}

% Hyperlinks
\usepackage[hidelinks]{hyperref}

% Glossary entries
\newglossaryentry{shift-invariant}{
  name=shift-invariant,
  description={Un kernel \`e shift-invariant se e solo se la sua trasformata di Fourier non cambia con la traslazione del contenuto dell'immagine}
}
\newglossaryentry{blind-deconvolution}{
  name=blind-deconvolution,
  description={Metodo di estrazione di ground-truth image in cui il blur kernel \`e ignoto}
}
\newglossaryentry{non-blind-deconvolution}{
  name=non-blind-deconvolution,
  description={Metodo di estrazione di ground-truth image in cui il blur kernel \`e noto}
}

% Title and author
\title{\textbf{Rete Convoluzionale per Image Deblurring}}
\author{Alessio Tanzi, Giovanni Tortia, Francesco Novia}
\date{AA2024/2025}

\begin{document}

\twocolumn[
  \maketitle
  \begin{abstract}
    L'obiettivo del progetto \`e l'applicazione di una U-Net Convoluzionale al fine di migliorare la qualit\`a dell'immagine in input rimuovendo il Blur causato dal moto del soggetto acquisito
    (\textit{Motion Blur}) o causato dalla messa a fuoco dell'obiettivo (\textit{Focus Blur}).
  \end{abstract}
  \vspace{1em}
]

\section{Introduzione}

Il restauro delle immagini è un compito fondamentale della visione artificiale, utile in campi come telerilevamento,
imaging medico e fotografia. Il compito consiste nel ricostruire un'immagine pulita da una versione degradata,
un problema noto per essere "mal posto".

Tradizionalmente, si sono usate tecniche basate su caratteristiche manuali, ma oggi il deep learning — in particolare CNN e Transformer —
ha rivoluzionato l'approccio.
L'obiettivo di questo lavoro è esplorare la architettura CNN "ConvIR"\cite{convir} capace di eguagliare o superare le prestazioni dei
modelli Transformer, mantenendo un numero minore di parametri e complessit\`a computazionale minore.
Di seguito le caratteristiche principali della rete in questione:

\textbf{Architettura multi-scala potenziata}: \mbox{ConvIR} imita i meccanismi multi-stage dei Transformer all'interno di una classica struttura a U (U-Net),
gestendo i blur da grossolani a fini.

\textbf{Modulo di attenzione multi-forma (MSA)}: Un nuovo modulo che aggrega informazioni in regioni quadrate e rettangolari,
usando anche \textit{convoluzioni dilatate} per aumentare il campo ricettivo.

\textbf{Modulazione della frequenza}: A differenza di altri metodi che trasformano le immagini nel dominio delle frequenze (es. Fourier),
ConvIR regola direttamente l'importanza dei segnali ad alta frequenza tramite pesi di attenzione.

\textbf{Efficienza computazionale}: ConvIR raggiunge prestazioni simili o superiori ai Transformer con meno parametri e FLOPs.

\textbf{Versioni del modello}\cite{convir}: Sono proposte tre varianti (Small, Base, Large), le quali determinano la lunghezza del primo blocco residuale
all'interno del \textit{CNNBlock}.

\textbf{Note su tecnologie correlate}
\begin{itemize}[topsep=0pt, noitemsep]
  \item CNN vs Transformer: I Transformer offrono vantaggi nel cogliere relazioni a lungo raggio ma sono pesanti computazionalmente.
  \item Moduli di attenzione: Varianti avanzate mirano a focalizzarsi su zone importanti dell'immagine.
  \item Elaborazione spettrale: ConvIR usa strategie alternative ai trasformatori di Fourier e wavelet, risparmiando tempo computazionale.
\end{itemize}

\section{Modello teorico e approcci tradizionali}

Una immagine con blur \`e modellata matematicamente come convoluzione tra immagine ground-truth latente e kernel di blur, dove quest'ultimo si assume essere \textit{\gls{shift-invariant}}. In questo caso,
l'estrazione dell'immagine sharp \`e un problema di \textit{Image Deconvolution}, la quale \`e suddivisa in \textit{\Gls{non-blind-deconvolution}} e \textit{\Gls{blind-deconvolution}}.\par
Formulazione Matematica:

\begin{math}
  \bm{b} = \bm{i} * \bm{k} + \bm{n}
\end{math}

Dove:

\begin{itemize}[topsep=0pt, noitemsep]
  \item[] $\bm{b}$: Immagine con blur
  \item[] $\bm{i}$: Immagine \textit{ground-truth} latente
  \item[] $\bm{k}$: Blur Kernel
  \item[] $\bm{n}$: Rumore presente nell'immagine per contare imperfezioni causate dall'acquisizione (quantizzazione, saturazione del colore, risposta non linare della camera, ...) (Esempio: rumore gaussiano)
\end{itemize}

\paragraph*{Non-Blind Deconvolution}

In questa metodologia tradizionale, il blur kernel \`e noto a priori (Esempio: Point Spread Function Gaussiana per Blur senza direzione, Linea con direzione e lunghezza per Blur con direzione).\par
Uno dei primi metodi utilizzati in questa categoria, implementato come comparazione, \`e la \textit{Wiener Deconvolution}, il cui obiettivo \`e la ricerca di un filtro $\bm{g}$ tale che, tramite
convoluzione con l'immagine blurred $\bm{b}$. Espresso nel dominio di Fourier:

\begin{align}
  \hat{\bm{I}} &= \bm{G}\bm{B} \\
  \bm{G}       &= \frac{|\bm{K}|^2}{|\bm{K}|^2+\frac{1}{\mathrm{SNR}}} \frac{1}{\bm{K}}
\end{align}

Dove:

\begin{itemize}[topsep=0pt, noitemsep]
  \item[] $\bm{G}$ e $\bm{K}$: trasformate di Fourier di $\bm{g}$ e $\bm{k}$
  \item[] $\mathrm{SNR}$: Signal to noise ratio (infinitamente alto se rumore assente)
\end{itemize}

Un'implementazione di tale metodo di Deblurring si basa su un metodo di ottimizzazione convessa chiamato \textit{Alternating Direction Method of Multipliers} (ADMM)\footnote{\url{https://stanford.edu/class/ee367/reading/lecture6_notes.pdf}}

\paragraph*{Blind Deconvolution}

In questa metodologia, il blur kernel \`e ignoto\footnote{I metodi con neural network rientrano in questa categoria}, dunque parte dell'algoritmo \`e la \textit{PSF estimation}, modellata come
stima di una stima di densit\`a di probabilit\`a.

\section{Architettura del modello}
L'architettura utilizzata in~\cite{convir} si basa, come detto, sulla struttura tipica di una U-Net con encoder-decoder convoluzionali, ed estrazione delle feature effettuata a risoluzioni multiple, quindi effettuando downsampling all'immagine di partenza.
Di rilevante importanza in tal senso è il ruolo dei \textit{Multi-Scale Modules} (\textit{MSM}), che hanno l'obiettivo di implementare meccanismi di attenzione di diversa forma (quadrata e rettangolare) e dimensione.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/architecture_complete.png}
  \caption{Schema generale dell'architettura completa (ConvIR)}
  \label{fig:architecture}
\end{figure*}

In figura~\ref{fig:architecture} è riportata l'architettura completa. La prima caratteristica interessante è che l'immagine in input viene elaborata non solo alla risoluzione primaria (256x256)
proveniente dal dataloader, ma viene ulteriormente sottocampionata (rispettivamente alla metà e a un quarto della risoluzione) e reinserita nella rete come input combinato dei layer successivi.
L'obiettivo di questa configurazione multi-input multi-output è di analizzare l'immagine degradata secondo diversi livelli di dettaglio, e quindi di individuare pattern e feature più variegati e di diversa intensità.

L'encoder e il decoder hanno una struttura pressoché speculare, con tre skip connection che collegano i due rami, corrispondenti alle tre differenti risoluzioni a cui viene elaborata l'immagine. Come avviene di consueto nelle reti convolutive,
al ridursi della dimensione del tensore in larghezza e altezza, cresce il numero di canali. La feature extraction passa infatti attraverso i seguenti moduli:
\begin{enumerate}[label=\textbf{(\alph*)}]
  \item layer convolutivo semplice (blocco \textit{Conv} in verde in figura);
  \item \textit{ConvS}, blocco utilizzato solo per le versioni sottocampionate dell'immagine in input: consiste in una sequenza di quattro layer convolutivi che mantengono costanti le dimensioni di larghezza e altezza;
  \item \textit{CNNBlock}, blocco costituito da una serie di layer convolutivi raggruppati in \textit{n+1} blocchi residuali; nell'ultimo di questi blocchi viene inserito anche il \textit{MSM};
  \item \textit{MSM (Multi-Scale Module)}: fonde l'elaborazione di tre moduli \textit{MSA} (\textit{Multi-Scale Attention}), che operano appunto su tre scale dimensionali gradualmente decrescenti.
    Ogni \textit{MSA} combina di fatto l'output di un \textit{DSA} e un \textit{DRA};
  \item \textit{DSA (Dilated Square Attention)}: produce prima una attention map concentrandosi sulle aree quadrate del tensore in input, attraverso layer convolutivi e di pooling,
    e in seguito la elabora attraverso un filtro passa alto con parametri allenabili che sottrae unicamente la componente continua e tende ad esaltare quelle a più alta frequenza,
    tipicamente responsabili del blur;
  \item \textit{DRA (Dilated Rectangle Attention)}: modulo analogo al precedente ma focalizzato su pattern di forma rettangolare, combina attention map in senso verticale e orizzontale.
\end{enumerate}

L'architettura originale proposta in~\cite{convir} è stata utilizzata dagli autori in tre diversi formati: \textit{S (Small)}, \textit{B (Base)} e \textit{L (Large)}, ognuna caratterizzata da un numero variabile di blocchi residuali all'interno di ciascun modulo \textit{CNNBlock}.
Le configurazioni prevedevano \(n=3\), \(n=7\) e \(n=15\) rispettivamente per le varianti \textit{S}, \textit{B} e \textit{L}.

La loss function utilizzata corrisponde alla somma pesata di un contributo calcolato nel dominio spaziale (\(\mathcal{L}_1\)) e uno in frequenza (\(\mathcal{L}_{freq}\)), in modo da considerare adeguatamente i diversi apporti dovuti alla presenza del blur:

\begin{align}
  \mathcal{L}_1 &= \mathcal{L}_{spatial} = \sum_{i=1}^{3}\frac{1}{P_i} \left\lVert \hat{\mathbf{I}}_i - \mathbf{Y}_i \right\rVert _1, \\
  \mathcal{L}_{freq} &= \sum_{i=1}^{3}\frac{1}{S_i} \left\lVert [\mathcal{R}(\hat{\mathbf{I}}_i), \mathcal{I}(\hat{\mathbf{I}}_i)] - [\mathcal{R}(\mathbf{Y}_i), \mathcal{I}(\mathbf{Y}_i)] \right\rVert _1,
\end{align}

Dove \(i\) indicizza gli output multipli a diverse risoluzioni; \(\hat{\mathbf{I}}\) e \(\mathbf{Y}\) rappresentano rispettivamente l'immagine elaborata dalla rete e il ground truth;
\(P\) e \(S\) indicano il numero totale di elementi dei tensori presi in considerazione, in modo da avere delle metriche normalizzate;
gli operatori \(\mathcal{R}()\) e \(\mathcal{I}()\) estraggono rispettivamente la parte reale e immaginaria della FFT operata sull'immagine.\\
La funzione di costo complessiva è così calcolata:

\begin{equation}
  \mathcal{L}_{tot} = \mathcal{L}_{spatial} + \lambda\mathcal{L}_{freq},
  \label{eq:total_loss_function}
\end{equation}

dove \(\lambda\) è un iperparametro impostato di default a \(0.01\).

\section{Sperimentazioni}

Si illustrano di seguito i dettagli del setup sperimentale adottato, includendo le alterazioni apportate sull'architettura originale e al processo di training, indicandone motivazioni e risultati riscontrati.

\paragraph{Dataset}
I dataset usati sono principalmente due: GOPRO\footnote{\url{https://www.kaggle.com/datasets/lqzmlaq/gopro-large}} e RSBlur\footnote{\url{https://drive.google.com/drive/folders/1sS8_qXvF4KstJtyYN1DDHsqKke8qwgnT}}.
GOPRO è un dataset creato applicando un motion blur artificiale ai frame di una serie di video girati all'aperto, RSBlur invece usa una speciale fotocamera per creare contemporaneamente
una versione sfocata e una nitida della stessa immagine.

Per cercare di ottenere un risultato migliore possibile, abbiamo condotto dei test su entrambi i dataset che risultassero in una singola metrica di performance complessiva, nello specifico:
\[ \text{Score} = \frac{1}{2}\sum_{i=1}^2 100 \cdot avg(\frac{PSNR_i}{33}, SSIM_i)\]
Dove $i$ indicizza il dataset su cui è stato svolto il test. Questa metrica è stata definita in modo da dare lo stesso peso alle metriche di SSIM e PSNR, oltre che ai due dataset: per ogni dataset si calcolano SSIM e PSNR medio, dopodichè si normalizza
il valore di PSNR in modo da riportarlo nell'intervallo [0, 1], si calcola la media tra PSNR e SSIM e la si moltiplica per 100, in modo da ottenere dei valori nell'intervallo [0, 100]; ottenuto questo valore quindi lo score è dato dalla media sui dataset.

Inizialmente il training è stato condotto usando GOPRO, ma dal punteggio è emerso come i modelli addestrati su RSBlur ottenessero una performance più bilanciata, mentre invece quelli addestrati su GOPRO tendevano a dare degli ottimi risultati sul test set, ma dei pessimi
risultati per quello che riguarda RSBlur. Nel corso dell'addestramento è stato provato come dataset anche un misto di RSBlur e GOPRO, con scarsi risultati, e alla fine si è deciso di usare esclusivamente RSBlur.

Il training set utilizzato consiste quindi in un sottoinsieme degli esempi di RSBlur, che comprende un totale di 11857 coppie di immagini. Sono stati prodotti diversi sottoinsiemi di RSBlur, compreso uno a risoluzione più bassa dell'originale, ottenuto tramite la funzione \texttt{resize(...)}\footnote{\url{https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html}} di OpenCV,
ma la versione finale consiste in un training set di 2755 esempi e un test set di 1000 esempi, ottenuto tramite crop delle immagini originale per riportare tutte le immagini alla stessa dimensione (invece che \texttt{resize} di OpenCV che interpola).

Oltre ai set citati, abbiamo anche usato un set di 32 immagini sfocate, senza versione nitida, scattato da noi, in modo da ottenere un riscontro più pratico e intuitivo (seppur non molto oggettivo). Nella sezione \ref{results} si possono trovare i risultati su alcune di queste immagini.

Ciascun dataset è stato suddiviso in \textbf{train/validation/test} secondo le seguenti modalità:
\begin{itemize}
    \item Il \textbf{test set} è separato nativamente nel dataset scaricato, poiché il \textit{dataloader} si aspetta la presenza delle due cartelle \texttt{train/} e \texttt{test/} nella directory fornita al programma.
    \item Il \textbf{validation set} è stato ottenuto estraendo il \textbf{15\% finale} delle immagini presenti nella cartella \texttt{train/}, ordinate in modo \textit{lessicografico}. Lo split non è casuale, al fine di garantire la \textbf{riproducibilità}.
\end{itemize}

\paragraph{Funzione di Training}
Ciascun Forward e Backward pass sono eseguiti, a fine di incrementare le prestazioni, con il formato floating point a 16 bit Brain Float,
sfruttando il meccanismo di troncamento automatico offerto da Pytorch chiamato \textit{AMP}\footnote{https://docs.pytorch.org/docs/stable/amp.html}.
Si nota che, in quanto in questo processo si vanno a perdere cifre decimali della mantissa, e dunque i contributi ai vari layer potrebbero risultare nulli,
la Loss viene scalata prima di applicare il backward pass con un $\mathrm{growth\_factor} = 2.0$

Per quanto riguarda la funzione di costo, abbiamo utilizzato quella descritta in precedenza (\ref{eq:total_loss_function}),
ma abbiamo aggiunto un termine di gradient penalty, in modo da disincentivare una crescita eccessiva dei gradienti, consentendo quindi una regolarizzazione più efficace del modello. \\
Il backward pass comprende quindi un primo calcolo dei gradienti, secondo la loss function originale (\ref{eq:total_loss_function}), si ottiene poi la penalità,
accumulando la norma euclidea di tutti i gradienti appena ricavati, e la si somma alla funzione di costo precedente, infine vengono propagati i gradienti a partire da quest'ultima loss complessiva.

\paragraph{Iperparametri}

Di seguito gli iperparametri utilizzati assieme al valore selezionato a seguito di una esplorazione trial-and-error:
\begin{itemize}[topsep=5pt, itemsep=7pt]
  \item \textbf{Batch Size}:
    \begin{itemize}[topsep=0pt, noitemsep]
      \item[\texttt{D}] Numero di Coppie di immagini, Example e Label, caricate in un ciclo di Stochastic Gradient Descent
      \item[\texttt{V}] \textbf{Valore}: $8$. Valori maggiori, eg. $12$ tendono a rendere estremamente lento un ciclo di forward e backward propagation,
        mentre valori superiori o uguali a $32$ Esauriscono la
        memoria del GPU device utilizzato per i test\footnote{RTX 3060, VRAM: 8GB}
    \end{itemize}

  \item
    \textbf{Optimizer}: L'ottimizzatore utilizzato \`e l'algoritmo di Adam\cite{kingma2017adammethodstochasticoptimization} con i seguenti parametri
    \begin{itemize}[topsep=0pt, noitemsep]
      \item[] $\beta_1 = 0.9$ peso del primo momento
      \item[] $\beta_2 = 0.999$ peso del secondo momento
    \end{itemize}

  \item
    \textbf{Learning Rate (Scheduling)}
    \begin{itemize}[topsep=0pt, noitemsep]
      \item[\texttt{D}] Moltiplicatore determinante Step Size dei parametri a partire dalla loss.
      \item[\texttt{V}] \textbf{Valore Iniziale}: $10^{-4}$. Circa la schedulazione, tra le strategie di schedulazione considerate,
        in particolare, \textit{Momentum}, \textit{Warmup}, \textit{Decay}, sono state impiegate la strategia di \textit{Gradual Warmup}
        per le prime $N - 3$ epoche, seguita da un \textit{Cosine Decay} nelle
        epoche rimanenti. Si nota che \`e stato evitato l'impiego del momento in quanto superfluo con ottimizzatori a learning
        rate adattivo.\footnote{Infatti, il calcolo del momento \`e incluso nell'Adam Optimizer} Sono stati testati valori più alti, come ad esempio $1.2\cdot 10^{-4}$ e $1.5 \cdot 10^{-4}$, ma questi 
        risultavano in differenze minime o addirittura a far divergere la loss.
    \end{itemize}

  \item
    \textbf{Weight Decay}:
    \begin{itemize}[topsep=0pt, noitemsep]
      \item[\texttt{D}] Nell'ambito dell'ottimizzatore Adam implementato da Pytorch, fattore moltiplicativo dei componenti del gradiente da
        ogni layer.  Quando \`e diverso da zero, ciascun componente \`e moltiplicato per $1 + \mathrm{weight\_decay}$
      \item[\texttt{V}] $0$
    \end{itemize}

  \item
    \textbf{Accumulate Gradient Frequency}:
    \begin{itemize}[topsep=0pt, noitemsep]
      \item[\texttt{D}] Frequenza, in termini di numero di batch processati, di applicazione dei gradienti calcolati dagli ultimi
        $\mathrm{agf}$ backward pass
      \item[\texttt{V}] $1$. Valori Maggiori (sono stati testati 2 e 3) tendono a rendere la curva della loss meno stabilmente decrescente
    \end{itemize}

  \item
    \textbf{$\lambda$}:
    \begin{itemize}[topsep=0pt, noitemsep]
      \item[\texttt{D}] Fattore di peso del contributo della Loss nel dominio di Fourier
      \item[\texttt{V}] $0.1$
    \end{itemize}
\end{itemize}

\paragraph{Data Augmentation}

\begin{figure*}[tb!]
  \centering
  \subfloat[\centering Metriche di validazione: SSIM (a sinistra) e PSNR (a destra) su epochs.]{
    \resizebox{0.4\linewidth}{!}{
      \begin{tikzpicture}
        \begin{axis}[
            width=0.9\linewidth,
            xlabel={epoch},
            ylabel={SSIM},
            xtick distance={400},
            enlarge x limits=false,
            enlarge y limits=false,
            grid=none,
            thick,
            axis y line*=left,
            axis x line*=bottom,
            legend style={
              at={(0.97,0.16)},
              anchor=south east,
              cells={align=left},
              font=\scriptsize
            }
          ]
          % SSIM on left y-axis
          \addplot[blue, thick] table[x index=0, y index=1] {figures/validation_metrics_ssim.txt};
          \addlegendentry{SSIM}
        \end{axis}

        % Second axis overlay for PSNR
        \begin{axis}[
            width=0.9\linewidth,
            xlabel={epoch},
            ylabel={PSNR},
            xtick=\empty, % don't duplicate xticks
            yticklabel style={text=red},
            ylabel style={text=red},
            axis y line*=right,
            axis x line=none,
            xmin=0, xmax=2000, % or whatever the full range is
            grid=none,
            thick,
            legend style={
              at={(0.97, 0.03)},
              anchor=south east,
              cells={align=left},
              font=\scriptsize
            }
          ]
          % PSNR on right y-axis
          \addplot[red, thick, dashed] table[x index=0, y index=1] {figures/validation_metrics_psnr.txt};
          \addlegendentry{PSNR}
        \end{axis}
      \end{tikzpicture}
    }
  }
  \hfill
  \subfloat[\centering Training loss nelle epoche]{
    \resizebox{0.4\linewidth}{!}{
      \begin{tikzpicture}[scale = 0.4]
        % Left axis: Content loss
        \begin{axis}[
            width=1.5\linewidth,
            xlabel={epoch},
            ylabel={Loss},
            tick label style={font=\scriptsize},  % Smaller x-axis labels
            xtick distance={400},
            %x tick label style={text width = 1.7cm, align = center, rotate = 70, anchor = north east}, % Optional: rotate for dense x values
            enlarge x limits=false,
            enlarge y limits=false,
            grid=major,
            xmajorgrids=false,
            ymajorgrids=true,
            axis y line*=left,
            axis x line*=bottom,
          ]
          \addplot[blue!60!black, thick] table[x index=0, y index=1] {figures/out.txt};
        \end{axis}
      \end{tikzpicture}
    }
  }

  \label{fig:metrics}
\end{figure*}

Sono mantenute pressoché invariate le trasformazioni usate dagli autori per la data augmentation, ovvero:
\begin{itemize}
  \item \texttt{RandomCrop(256)}: Estrae un crop quadrato di lato 256 pixel, al fine di poter allenare la rete su pi\`u porzioni dell'immagine, presentando
    blur pattern diversi alla rete.
  \item \texttt{RandomHorizontalFlip(p=0.5)}: Decide casualmente (con una probabilità del 50\%) se ruotare intorno all'asse verticale il crop ottenuto al passo precedente.
\end{itemize}

Sono inoltre state introdotte altre trasformazioni alteranti le informazioni sul colore dell'immagine in input, come \texttt{ColorJitter},
e \texttt{AutoAugment} (\cite{cubuk2019autoaugmentlearningaugmentationpolicies}) con policy IMAGENET. Queste trasformazioni menzionate sono state in seguito rimosse a seguito di riscontro di performance degradata.
Tale risultato trova giustifica nel fatto che modificare il contenuto dei pixel delle immagini si altera l'informazione associata al blur contenuto in esse.


\paragraph{Alterazioni sull'architettura}
Sono stati effettuati diversi tentativi per ottimizzare la struttura architetturale originale in modo da snellire le tempistiche e il carico computazionale per l'addestramento ma mantenendo comunque delle
prestazioni comparabili, per quanto inferiori, all'originale e dei risultati visivamente apprezzabili.

Abbiamo ad esempio provato a sostituire, all'interno del ramo di decoding, l'operazione di deconvoluzione (o convoluzione trasposta) con quella di pixel shuffle (introdotta in~\cite{shi2016realtimesingleimagevideo}), 
sulla carta più efficiente per operazioni di upsampling; in realtà, questo tentativo non ha portato miglioramenti significativi, per cui non abbiamo mantenuto questa modifica.

In definitiva, abbiamo deciso di intervenire sul modulo \textit{CNNBlock}, impostando numero di blocchi residuali pari a \(\mathbf{n=6}\) in ciascun modulo, generando una sorta di variante intermedia tra la \textit{S} 
e la \textit{B}. Abbiamo inoltre rimosso completamente il modulo \textit{MSM} al suo interno: sebbene si trattasse di uno dei componenti caratterizzanti dell'architettura di partenza, dati i meccanismi di attenzione implementati,
abbiamo osservato e valutato, anche grazie agli studi in~\cite{convir}, che in sostanza andava ad appesantire considerevolmente il modello introducendo dei miglioramenti marginali e difficilmente percepibili,
soprattutto considerati gli obiettivi fissati per questo progetto.

Un'ultima variazione ha previsto l'introduzione della funzione di attivazione GELU (Gaussian Error Linear Unit) al posto della ReLU per tutti i layer convolutivi, che conferisce una convergenza migliore e riduce la 
possibilità di scomparsa dei gradienti.

\paragraph{Possibili casi critici}
Nel corso degli esperimenti sono state trovate due criticità: la rete, infatti, tende a lavorare peggio con immagini ad alta risoluzione (e.g. $4080\times3060$), lasciandole molto più invariate rispetto alla stessa immagine a risoluzione più bassa come ad esempio $1280\times720$ o $1920\times1080$.
 Il motivo preciso per cui questo accada non è stato individuato, ma sospettiamo che abbia a che vedere con la mancanza dei meccanismi di attenzione implementati nel paper originale: i kernel convolutivi usati infatti potrebbero coprire aree troppo piccole dell'immagine 
 per poter rilevare certe zone sfocate che in immagini ad alta risoluzione sono composte da molti pixel.

\section{Risultati e conclusioni finali}\label{results}

Nel presente lavoro, i risultati del modello addestrato sono stati confrontati con quelli della versione originale di ConvIR~\cite{convir}. Sebbene le prestazioni siano complessivamente inferiori rispetto ai modelli basati su Transformer, il nostro approccio risulta competitivo in alcuni casi, con metriche comparabili (soprattutto SSIM) e una significativa riduzione della complessità architetturale.

La riduzione dei parametri è cruciale: ottenuta intervenendo sul numero di blocchi convoluzionali e rimuovendo moduli onerosi come il Multi-Scale Module. Ulteriori strategie di compressione, come l'uso sistematico di convoluzioni $1 \times 1$ in stile NiN (introdotte in GoogLeNet~\cite{szegedy2014goingdeeperconvolutions}), potrebbero ridurre ulteriormente la dimensione del modello, fungendo da bottleneck tra layer convolutivi.

Per l'upscaling nel decoder, l'uso di \textit{PixelShuffle}~\cite{zamzam2025pixelshufflersimpleimagetranslation} rappresenta un'alternativa priva di parametri apprendibili rispetto alle deconvoluzioni, utile per progettare modelli leggeri.

Nel complesso, i risultati mostrano un buon compromesso tra efficienza e qualità dell'output: nonostante una lieve perdita in accuratezza, l'approccio proposto offre vantaggi in termini di tempo di addestramento, risorse e velocità di inferenza.

Sono riportati esempi qualitativi con: (1) immagine originale sfocata, (2) output deblurred, (3) residuo applicato (inteso come il modulo della differenza algebrica tra tensori in input e di output), amplificato per evidenziare le correzioni. Inoltre, vengono presentati:

\begin{itemize}
  \item grafici di SSIM e PSNR sui dati di validazione;

  \item grafico della funzione di costo nel tempo;

  \item tabelle riepilogative delle metriche finali.
\end{itemize}

I risultati confermano la validità dell’approccio, mostrando che anche modelli leggeri e meno complessi possono garantire prestazioni visivamente e numericamente soddisfacenti rispetto a quelli basati su self-attention.

\FloatBarrier

\begin{figure*}[b!]

  \begin{tabular}{|l|c|c|c|c|c|}
  \hline
  \textbf{Model} & \textbf{RSBlur PSNR} & \textbf{RSBlur SSIM} & \textbf{GOPRO PSNR} & \textbf{GOPRO SSIM} & \textbf{\#Parameters} \\
  \hline
  ConvIR-L & 34.06 dB & 0.868 & 33.28 dB & 0.963 & 14.83M \\
  Ours    & 30.95 dB & 0.83 & 26.72 dB & 0.85 & 5.35M \\
  \hline
  \end{tabular}

  \begin{mdframed}[
      linewidth=1pt,
      innertopmargin=6pt,
      innerbottommargin=6pt,
      innerleftmargin=6pt,
      innerrightmargin=6pt
      linecolor=black,
      nobreak
    ]

    \centering
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_184657.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_184657_deblurred.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/comparison_20250503_184657.jpg}
    }\hspace{0.1cm}\hfill
    \\ \vspace{0.3cm}
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_184720.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_184720_deblurred.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/comparison_20250503_184720.jpg}
    }\hspace{0.1cm}\hfill
    \\ \vspace{0.3cm}
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_185104.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_185104_deblurred.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/comparison_20250503_185104.jpg}
    }\hspace{0.1cm}\hfill
    \\ \vspace{0.3cm}
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_185233.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/20250503_185233_deblurred.jpg}
    }\hspace{0.1cm}\hfill
    \subfloat{
      \centering\includegraphics[width=0.31\linewidth]{figures/sample_imgs/comparison_20250503_185233.jpg}
    }\hspace{0.1cm}\hfill
    \\ \vspace{0.3cm}

    \vspace{3pt}
    \hrule
    \vspace{3pt}
    \caption{
      Una serie di immagini prodotte dal modello: nella prima colonna è presente l'immmagine originale (sfocata), nella seconda l'output della rete,
      e nella terza il residuo che viene sommato all'immagine originale per ottenere la versione nitida,
      il cui valore dei pixel è stato raddoppiato in modo da rendere più visibili le zone interessate dalla correzione dell'immagine
    }
  \end{mdframed}
  \label{foto_esempio}
\end{figure*}

\FloatBarrier

\printglossary[title=Glossario, toctitle=Glossario]

\printbibliography

\end{document}
